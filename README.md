# ğŸ§  ASL Alphabet Recognition Using Deep Learning

---

## ğŸ“Œ Overview

This project uses deep learning and computer vision to recognize the American Sign Language (ASL) alphabet. It supports both **static hand gestures (Aâ€“Z)** and **dynamic motion gestures (J & Z)**.

It was built to demonstrate my skills in AI model training, data preprocessing, real-time inference, and the application of machine learning to real-world accessibility challenges.

---

## ğŸŒ Real-World Impact

Millions of people globally rely on sign language to communicate. However, language barriers often persist between Deaf and hearing communities. This project aims to:

- Bridge that communication gap using AI
- Support assistive technology tools
- Enable more inclusive and accessible communication interfaces
- Serve as a foundation for educational or accessibility-focused applications

---

## ğŸ§ª Tech Stack

- **Language:** Python  
- **Frameworks:** TensorFlow, Keras, OpenCV  
- **Data Handling:** Pandas, NumPy  
- **Visualization:** Matplotlib  
- **Interface:** Jupyter Notebook, OpenCV video feed

---

## ğŸ“ Project Structure

```bash
asl_recognition/
â”‚
â”œâ”€â”€ data/ # Downloaded datasets
â”‚ â”œâ”€â”€ asl_alphabet_train/ # Aâ€“Z static image dataset
â”‚ â””â”€â”€ jz_videos/ # J and Z dynamic gesture videos
â”‚
â”œâ”€â”€ models/ # Trained models and encoder
â”‚ â”œâ”€â”€ asl_dynamic_ml.keras # J and Z motion model
â”‚ â”œâ”€â”€ asl_letter_mlp.keras # Aâ€“Z static model
â”‚ â””â”€â”€ label_encoder.pkl 
â”‚
â”œâ”€â”€ utils/ # Utility scripts
â”‚ â”œâ”€â”€ extract_dynamic_landmarks.py
â”‚ â”œâ”€â”€ extract_landmarks.py
â”‚
â”œâ”€â”€ real_time_inference.py # Program Entry Point
â”œâ”€â”€ recorded_dynamic.py 
â”œâ”€â”€ train_dynamic_model.py # Model training script (J & Z)
â”œâ”€â”€ train_model.py # Model training script (Aâ€“Z)
â”œâ”€â”€ requirements.txt 
â””â”€â”€ README.md
``` 
---

## ğŸ“¥ Dataset Instructions

### ğŸ“¦ 1. Download Datasets

- **Static Aâ€“Z images:**  
  [ASL Alphabet Dataset (Kaggle)](https://www.kaggle.com/datasets/grassknoted/asl-alphabet)

- **Dynamic J & Z videos:**  
  [ASL Motion Dataset (Kaggle)](https://www.kaggle.com/datasets/signnteam/asl-sign-language-alphabet-videos-j-z)

> ğŸ” *Make sure youâ€™re logged in to Kaggle and have set up the Kaggle API.*

---

### ğŸ”½ 2. Download via Kaggle CLI (Recommended)
```bash
pip install kaggle
```
Place your kaggle.json in ~/.kaggle. and run:
```bash
# Static images
kaggle datasets download -d grassknoted/asl-alphabet
unzip asl-alphabet.zip -d data/

# Dynamic gestures (J & Z)
kaggle datasets download -d signnteam/asl-sign-language-alphabet-videos-j-z
unzip asl-sign-language-alphabet-videos-j-z.zip -d data/jz_videos/
```

---

### âš™ï¸ Setup & Usage
1. Clone the repository
```bash
git clone https://github.com/RyanChessell/asl_recognition.git
cd asl_recognition
```
2. Create & Active Virtual Enviroment
On WSL / Linux / macOS:
```bash
python3 -m venv venv
source venv/bin/activate
```
On Windows PowerShell:
```bash
python -m venv venv
.\venv\Scripts\Activate.ps1
```

4. Install Dependencies
```bash
pip install -r requirements.txt
```

4. Train the Models
```bash
python scripts/train_dynamic_model.py
```
5. Run Real-Time ASL Recognition
```bash
python .\scripts\real_time_inference.py
```

---

### âš™ï¸ Future Improvements
- Integrate LSTM/3D-CNN for dynamic video gesture recognition (J, Z)

- Support full ASL word recognition and sentence generation

- Add voice feedback for real-time spoken output

- Deploy as a mobile/web app (via TensorFlow Lite or TF.js)

- Train on more diverse hand shapes and lighting conditions

### ğŸ“œ License
This project is licensed under the MIT License.

### ğŸ“« Contact
Ryan Chessell
[GitHub](https://github.com/RyanChessell)
[LinkedIn](www.linkedin.com/in/ryanchessell)
Email: ryans.chessell@gmail.com




